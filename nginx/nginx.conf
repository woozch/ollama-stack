events {}

http {
    # For LLM, increase timeout margin (global settings)
    proxy_connect_timeout 300s;   # timeout for connecting to the backend (ollama)
    proxy_send_timeout    300s;   # timeout for sending request body to the backend
    proxy_read_timeout    300s;  # timeout for waiting for response from the backend (LLM core)
    send_timeout          300s;  # timeout for sending response to the client
    
    server {
        # 11435 port for HTTPS
        listen 11435 ssl;
        server_name _;

        ssl_certificate     /etc/nginx/certs/server.crt;
        ssl_certificate_key /etc/nginx/certs/server.key;

        # Optional security settings
        # ssl_protocols TLSv1.2 TLSv1.3;
        # ssl_ciphers HIGH:!aNULL:!MD5;

        location / {
            # proxy to ollama service in docker compose network
            proxy_pass http://ollama:11434;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;
        }
    }
}
